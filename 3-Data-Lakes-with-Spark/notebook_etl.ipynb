{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "song_input_data = \"s3a://udacity-dend/song-data/A/A/A/\"\n",
    "log_input_data = \"s3a://udacity-dend/log-data/2018/11\"\n",
    "output_data = \"s3a://aws-emr-resources-926236161117-us-west-2/spark-dwh\"\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    ".config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    ".getOrCreate()\n",
    "\n",
    "# get filepath to song data file\n",
    "song_data_path = os.path.join(song_input_data,\"*.json\")\n",
    "\n",
    "# read song data file\n",
    "song_df = spark.read.json(song_data_path)\n",
    "# extract columns to create songs table\n",
    "song_table = song_df.select(\n",
    "    \"song_id\",\n",
    "    \"title\",\n",
    "    \"artist_id\",\n",
    "    \"year\",\n",
    "    \"duration\").distinct()\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "song_table_path = os.path.join(output_data,\"songs\")\n",
    "#song_table.write.mode(\"overwrite\").partitionBy(\"year\",\"artist_id\").parquet(song_table_path)\n",
    "# extract columns to create artists table\n",
    "artist_table = song_df.select(\n",
    "    \"artist_id\", \n",
    "    \"artist_name\", \n",
    "    \"artist_location\", \n",
    "    \"artist_latitude\", \n",
    "    \"artist_longitude\"\n",
    ").distinct()\n",
    "\n",
    "# write artists table to parquet files\n",
    "artist_table_path = os.path.join(output_data,\"artist\")\n",
    "#artist_table.write.mode(\"overwrite\").parquet(artist_table_path)\n",
    "\n",
    "# get filepath to log data file\n",
    "log_data_path = os.path.join(log_input_data,\"*.json\")\n",
    "# read log data file\n",
    "log_df = spark.read.json(log_data_path)\n",
    "\n",
    "# filter by actions for song plays\n",
    "log_df = log_df.filter(log_df.page == 'NextSong')\n",
    "# extract columns for users table    \n",
    "log_df.createOrReplaceTempView('logs')\n",
    "user_table = spark.sql('SELECT DISTINCT userId, firstName, lastName, gender, level FROM logs')\n",
    "\n",
    "# write users table to parquet files\n",
    "user_table_path = os.path.join(output_data,\"users\")\n",
    "#user_table.write.mode(\"overwrite\").parquet(os.path.join(user_table_path))\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: datetime.fromtimestamp( (x/1000.0) ), T.TimestampType()) \n",
    "log_df = log_df.withColumn(\"timestamp\", get_timestamp(log_df.ts))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = log_df.select(\"ts\",\"timestamp\")\n",
    "time_table.createOrReplaceTempView('time_view')\n",
    "time_table = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    ts,\n",
    "    timestamp,\n",
    "    day(timestamp) AS day,\n",
    "    month(timestamp) AS month,\n",
    "    year(timestamp) AS year FROM time_view\"\"\")\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table_path = os.path.join(output_data,\"time\")\n",
    "#time_table.write.mode(\"overwrite\").parquet(time_table_path)\n",
    "# read in song data to use for songplays table\n",
    "song_data_path = os.path.join(song_input_data,\"*.json\")\n",
    "song_df = spark.read.json(song_data_path)\n",
    "log_df.createOrReplaceTempView('log_view')\n",
    "song_df.createOrReplaceTempView('song_view')\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplay_table = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      row_number() OVER (ORDER BY l.ts) AS songplay_id,\n",
    "      l.timestamp AS start_time,\n",
    "      l.userId AS user_id,\n",
    "      l.level,\n",
    "      s.song_id,\n",
    "      s.artist_id,\n",
    "      l.sessionId AS session_id,\n",
    "      l.location,\n",
    "      l.userAgent,\n",
    "      month(timestamp) as month,\n",
    "      year(timestamp) as year\n",
    "    FROM log_view l\n",
    "    LEFT JOIN song_view s \n",
    "        ON l.artist = s.artist_name\n",
    "        AND l.song = s.title\"\"\")\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplay_table_path = os.path.join(output_data,\"songplays\")\n",
    "#songplay_table.write.mode(\"overwrite\").parquet(songplay_table_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
